/* This is a C/C++ precompiling header
 * DOC C++ 17
 * DATE 2020-12-19 2:47 PM
 * COMP MSVC Windows 10
 * AUTHOR Liao
 * ~ God bless no bug ~
 */

#pragma once
#define NEUNET_BEGIN namespace neunet {
#define NEUNET_END  }

NEUNET_BEGIN
// Neural Network Main
// BP Algorithm
/* Propagation layer calculation
 * - Parameter
 * vecInput     [Input]    Input vecotr     -
 * vecWeight    [Input]    Weight vector    -
 * vecOutputAdj [Input]    Adjusted vector  -
 * - Return
 * Signal output vector
 */
Matrix::matrix ForwProp(Matrix::matrix &vecInput, Matrix::matrix &vecWeight)
{
    auto vecOutput = vecWeight * vecInput;
    return vecOutput;
}
Matrix::matrix DerivativeErr(Matrix::matrix &vecInput, Matrix::matrix(*funcActDv)(Matrix::matrix&))
{
    return funcActDv(vecInput);
}
Matrix::matrix PreErr(Matrix::matrix &vecCurrErr, Matrix::matrix &vecWeight)
{
    auto vecPreErrGrad = Matrix::matrix::transposition(vecWeight) * vecCurrErr;
    return vecPreErrGrad;
}
Matrix::matrix WeightGrad(Matrix::matrix &vecCurrErr, Matrix::matrix &vecInput)
{
    auto vecWeightGrad = vecCurrErr * Matrix::matrix::transposition(vecInput);
    return vecWeightGrad;
}
/* Convolution Neural Network (CNN)
 * 1 - Activation function definition
 * 2 - Calculation
 * 3 - Back propagation of CNN
 */
/* Whether the kernel fit to input
 * - Parameter
 * nKernelLnCnt     [Input] Kernel line count           -
 * nKernelColCnt    [Input] Kernel column count         -
 * nInputLnCnt      [Input] Input vector line count     -
 * nInputColCnt     [Input] Inoput vector column count  -
 * nLnStride        [Input] Line stride                 -
 * nColStride       [Input] Column stride               -
 * - Return
 * [true]   Size valid
 * [false]  Size invalid
 */
bool IsKernelSize(uint64_t nKernelLnCnt, uint64_t nKernelColCnt, uint64_t nInputLnCnt, uint64_t nInputColCnt, uint64_t nLnStride, uint64_t nColStride)
{
    if (!((nInputColCnt - nKernelColCnt) % nColStride) &&
        !((nInputLnCnt - nKernelLnCnt) % nLnStride)) return true;
    else return false;
}
/* Get Convolution output vector's line&column count
 * - Parameter
 * vecInputLnCnt    [Input] Input vector column count -
 * vecInputColCnt   [Input] Inputvector column count  -
 * nKernelLnCnt     [Input] Kernel line count         -
 * nKernelColCnt    [Input] Kernel column count       -
 * nLnStride        [Input] Line stride               -
 * nColStride       [Input] Column stride             -
 * - Return
 * .first   Line metrics
 * .second  Column metrics
 */
std::pair<uint64_t, uint64_t> GetConvOutputVecLnColCnt(uint64_t vecInputLnCnt, uint64_t vecInputColCnt, uint64_t nKernelLnCnt, uint64_t nKernelColCnt, uint64_t nLnStride, uint64_t nColStride)
{
    /* Convoluted vector's size
     * X - Input size
     * K - Kernel size
     * S - Stride
     * Y - Output size
     * Y = (X - K) / S + 1
     */
    auto nOutputVecLnCnt = (vecInputLnCnt - nKernelLnCnt) / nLnStride + 1;
    auto nOutputVecColCnt = (vecInputColCnt - nKernelColCnt) / nColStride + 1;
    return std::make_pair(nOutputVecLnCnt, nOutputVecColCnt);
}
/* Get convoluted vector
 * - Parameter
 * vecSrc           [Quote] Operation vector        -
 * nKernelLnCnt     [Input] Kernel  line count      -
 * nKernelColCnt    [Input] Kernel  column count    -
 * nLnStride        [Input] Line stride             -
 * nColStride       [Input] Column stride           -
 * - Return
 * .first               Convoluted vector
 * .second              Traced vector's position
 * .second[m][n]        Convoluted vector's line m column n's position of original vector pair
 * .second[m][n].first  Convoluted vector's line m column n's position of original vector's line position
 * .second[m][n].second Convoluted vector's line m column n's position of original vector's column postion
 */
std::pair<Matrix::matrix, algo_queue<algo_queue<std::pair<uint64_t, uint64_t>>>> GetConvVec(Matrix::matrix &vecSrc, uint64_t nKernelLnCnt, uint64_t nKernelColCnt, uint64_t nLnStride, uint64_t nColStride)
{
    if (IsKernelSize(nKernelLnCnt, nKernelColCnt, vecSrc.get_line(), vecSrc.get_column(), nLnStride, nColStride))
    {
        auto pairOutputVecStat = GetConvOutputVecLnColCnt(vecSrc.get_line(), vecSrc.get_column(), nKernelLnCnt, nKernelColCnt, nLnStride, nColStride);
        auto nOutputVecLnCnt = pairOutputVecStat.first;
        auto nOutputVecColCnt = pairOutputVecStat.second;
        // For each convoluted calculation, there would be a line of element joining the convoluted input vector.
        auto nConvVecLnCnt = nOutputVecLnCnt * nOutputVecColCnt;
        // The quantity of column is equal to the kernel element amount.
        auto nConvVecColCnt = nKernelLnCnt * nKernelColCnt;
        auto vecConvVec = Matrix::matrix(nConvVecLnCnt, nConvVecColCnt);
        // Line location sequence
        algo_queue<algo_queue<std::pair<uint64_t, uint64_t>>> mapConvLocSeq(nConvVecLnCnt);
        for(auto i=0; i<nConvVecLnCnt; i++) mapConvLocSeq[i].init(nConvVecColCnt);
        auto vecConvFillLnCnt = 0;
        for(auto i=0; i+nKernelLnCnt<=vecSrc.get_line(); i+=nLnStride)
            for(auto j=0; j+nKernelColCnt<=vecSrc.get_column(); j+=nColStride)
            {
                // Kernel
                auto vecConvFillColCnt = 0;
                for(auto k=0; k<nKernelLnCnt; k++)
                    for(auto l=0; l<nKernelColCnt; l++)
                    {
                        auto currLnPos = i + k, currColPos = j + l;
                        mapConvLocSeq[vecConvFillLnCnt][vecConvFillColCnt].first = currLnPos;
                        mapConvLocSeq[vecConvFillLnCnt][vecConvFillColCnt].second = currColPos;
                        vecConvVec[vecConvFillLnCnt][vecConvFillColCnt ++] = vecSrc[currLnPos][currColPos];
                    }
                vecConvFillLnCnt ++;
            }
        return std::make_pair(vecConvVec, mapConvLocSeq);
    }
    // Blank return
    else
    {
        algo_queue<algo_queue<std::pair<uint64_t, uint64_t>>> queueBlank;
        return std::make_pair(Matrix::matrix(), queueBlank);
    }
}
/* - Overload
 * For non-input vector
 * - Parameter
 * vecSrc   [Input] Vector  source  -
 * - Return
 * Non-convoluter original input vector
 */
Matrix::matrix GetConvVec(Matrix::matrix &vecSrc)
{
    return Matrix::matrix::reshape(vecSrc, vecSrc.get_line() * vecSrc.get_column(), 1);
}
/* Output back convolution
 * - Parameter
 * vecOutput        [Quote] Convoluted output vector    -
 * vecInputLnCnt    [Input] Inputvector line count      -
 * vecInputColnCnt  [Input] Inputvector column count    -
 * nKernelLnCnt     [Input] Kernel line count           -
 * nKernelColCnt    [Input] Kernel column count         -
 * nLnStride        [Input] Line stride                 -
 * nColStride       [Input] Column stride               -
 * - Return
 * Convoluted vector's original vector
 */
Matrix::matrix GetConvOutputOriginVec(Matrix::matrix &vecOutput, uint64_t vecInputLnCnt, uint64_t vecInputColnCnt, uint64_t nKernelLnCnt, uint64_t nKernelColCnt, uint64_t nLnStride, uint64_t nColStride)
{
    auto pairLnColCnt = GetConvOutputVecLnColCnt(vecInputLnCnt, vecInputColnCnt, nKernelLnCnt, nKernelColCnt, nLnStride, nColStride);
    auto nLnCnt = 0;
    Matrix::matrix vecConvOutputOrigin(pairLnColCnt.first, pairLnColCnt.second);
    for (auto i = 0; i < pairLnColCnt.first; i++)
        for (auto j = 0; j < pairLnColCnt.second; j++)
            vecConvOutputOrigin[i][j] = vecOutput[nLnCnt++][0];
    return vecConvOutputOrigin;
}
/* - Overload
 * Output back convolution
 * - Parameter
 * vecOutput    [Quote] Convoluted output vector    -
 * vecStdSize   [Quote] Standard vector             -
 * - Return
 * Convoluted vector's original vector
 */
Matrix::matrix GetConvOutputOriginVec(Matrix::matrix &vecOutput, Matrix::matrix &vecStdSize)
{
    return Matrix::matrix::reshape(vecOutput, vecStdSize.get_line(), vecStdSize.get_column());
}
/* CNN propagation operation
 * Step 1 - Vector convolution calculation
 * Step 2 - Pooling
 * Step 3 - Fully connected network operation
 */
// Forward propagation
Matrix::matrix ConvForwProp(Matrix::matrix &vecInput, Matrix::matrix &vecKernel, uint64_t nStride)
{
    if (IsKernelSize(vecKernel.get_line(), vecKernel.get_column(), vecInput.get_line(), vecInput.get_column(), nStride, nStride))
    {
        auto vecConvInput = GetConvVec(vecInput, vecKernel.get_line(), vecKernel.get_column(), nStride, nStride);
        auto vecConvKernel = GetConvVec(vecKernel);
        auto vecConvSigOutput = vecConvInput.first * vecConvKernel;
        auto vecSigOutput = GetConvOutputOriginVec(vecConvSigOutput, vecInput.get_line(), vecInput.get_column(), vecKernel.get_line(), vecKernel.get_column(), nStride, nStride);
        return vecSigOutput;
    }
    // Exception
    else return Matrix::matrix();
}
/* Merge the whole channels of a image vector
 * - Parameter
 * mapVecChans  [Quote] Vector's channels   -
 * - Return
 * Merged vector
 */
Matrix::matrix MergeChannel(algo_queue<Matrix::matrix> &mapVecChans)
{
    Matrix::matrix vecMerged;
    for(auto i=0; i<mapVecChans.size(); i++)
    {
        if(vecMerged.is_matrix()) vecMerged += mapVecChans[i];
        else vecMerged = mapVecChans[i];
    }
    return vecMerged;
}
algo_queue<Matrix::matrix> ConvForwProp(algo_queue<Matrix::matrix> &vecInput, algo_queue<Matrix::matrix> &vecKernel, uint64_t nStride)
{
    if(vecInput.size() == vecKernel.size())
    {
        algo_queue<Matrix::matrix> vecSigOutput(vecInput.size());
        for(auto i=0; i<vecInput.size(); i++) vecSigOutput[i] = ConvForwProp(vecInput[i], vecKernel[i], nStride);
        return vecSigOutput;
    }
    else return algo_queue<Matrix::matrix>();
}
/* Average Pooling operation
 * - Parameter
 * vecInput         [Quote] After padding Input  vector     -
 * nFilterLnCnt     [Input] Line directed filter size       -
 * nFilterColCnt    [Input] Column directed filter szie     -
 * bAvgPool         [Input] Average pooling operation flag  true
 * - Return
 * Pooling vector
 */
Matrix::matrix PoolForwProp(Matrix::matrix &vecInput, uint64_t nFilterLnCnt, uint64_t nFilterColCnt, bool bAvgPool = true)
{
    if (vecInput.get_line() % nFilterLnCnt == 0 && vecInput.get_column() % nFilterColCnt == 0)
    {
        auto vecConvInput = GetConvVec(vecInput, nFilterLnCnt, nFilterColCnt, nFilterLnCnt, nFilterColCnt);
        Matrix::matrix vecPoolConv(vecConvInput.first.get_line(), 1);
        for (auto i = 0; i < vecConvInput.first.get_line(); i++)
        {
            double dPoolVar = 0;
            uint64_t nLoc = 0;
            if(bAvgPool) dPoolVar = vecConvInput.first.get_avg_value(i, false);
            else dPoolVar = vecConvInput.first.get_max_value(i, nLoc, false);
            vecPoolConv[i][0] = dPoolVar;
        }
        auto vecConvOutputOrigin = GetConvOutputOriginVec(vecPoolConv, vecInput.get_line(), vecInput.get_column(), nFilterLnCnt, nFilterColCnt, nFilterLnCnt, nFilterColCnt);
        return vecConvOutputOrigin;
    }
    return Matrix::matrix();
}
algo_queue<Matrix::matrix> PoolForwProp(algo_queue<Matrix::matrix> &mapInput, uint64_t nFilterLnCnt, uint64_t nFilterColCnt, bool bAvgPool = true)
{
    algo_queue<Matrix::matrix> mapOutput(mapInput.size());
    for(auto i=0; i<mapInput.size(); i++)
        mapOutput[i] = PoolForwProp(mapInput[i], nFilterLnCnt, nFilterColCnt, bAvgPool);
    return mapOutput;
}
/* - Overload
 * Convert Previous layer's output vectors sequnce which is filled by atom vectors into a column Vector
 * - Parameter
 * mapAtomOutput    [Quote] Previous layer's activate output vector sequence    -
 * - Return
 * Fully connected network layer's input vetor
 */
Matrix::matrix CNNFCNIO(algo_queue<Matrix::matrix> &mapAtomOutput)
{
    auto nDimCnt = mapAtomOutput.size();
    Matrix::matrix vecFCNInput(nDimCnt, 1);
    for(auto i=0; i<nDimCnt; i++)
        if(mapAtomOutput[i].is_atom()) vecFCNInput[i][0] = mapAtomOutput[i].get_atom_vec();
        else
        {
            vecFCNInput.destroy();
            return Matrix::matrix();
        }
    return vecFCNInput;
}
/* For Fully connected network I/O operation
 * - Parameter
 * vecBackErr   [Quote] Error   -
 * - Return
 * Errors sequence transmitted back from fully connected network layer
 */
algo_queue<Matrix::matrix>CNNFCNIO(Matrix::matrix &vecBackErr)
{
    algo_queue<Matrix::matrix> mapBackErrSeq(vecBackErr.get_line());
    for(auto i=0; i<vecBackErr.get_line(); i++) mapBackErrSeq[i] = Matrix::matrix(vecBackErr[i][0]);
    return mapBackErrSeq;
}
Matrix::matrix GaussConnForwProp(Matrix::matrix &vecInput, Matrix::matrix &vecGaussLayerWeight)
{
    auto vecSigOutput = vecGaussLayerWeight * vecInput;
    return softmax(vecSigOutput);
}
/* CNN back propagation operation
 * Step 1 - Get each layer's error
 * Step 2 - Get the adjusted value of weight from each layer
 * Step 3 - New weight(Kernel)
 */
// CNN output layer back propagation
std::pair<Matrix::matrix, Matrix::matrix> GaussConnPreErr(Matrix::matrix &vecInput, Matrix::matrix &vecOutput, Matrix::matrix &vecOrigin, Matrix::matrix &vecGaussLayerWeight)
{
    auto vecCurrErr = softmax_derivative(vecOutput, vecOrigin);
    auto vecGaussLayerPreErrGrad = Matrix::matrix::transposition(vecGaussLayerWeight) * vecCurrErr;
    auto vecGaussLayerWeightGrad = vecCurrErr * Matrix::matrix::transposition(vecInput);
    return std::make_pair(vecGaussLayerPreErrGrad, vecGaussLayerWeightGrad);
}
/* Get convolution error
 * - Parameter
 * vecPreErr        [Quote] Next layer's error          -
 * vecInput         [Quote] Input vector                -
 * nFilterLnCnt     [Input] Line directed filter size   -
 * nFilterColCnt    [Input] Column directed filter size -
 * bIsMaxPool       [Input] Judge maximum pooling       true
 * - Return
 * Convoluted layer's error
 */
Matrix::matrix PoolBackProp(Matrix::matrix &vecErr, Matrix::matrix &vecInput, uint64_t nFilterLnCnt, uint64_t nFilterColCnt, bool bIsAvgPool = true)
{
    Matrix::matrix vecPoolErr(vecInput.get_line(), vecInput.get_column());
    auto nFilterLnLoc = 0, nFilterColLoc = 0;
    auto vecConvErr = GetConvVec(vecErr);
    // Iterate the pooling error elements
    for (auto i = 0; i < vecConvErr.get_line(); i++)
    {
        // Average Pooling
        if (bIsAvgPool)
        {
            // Error's average value
            auto dAvg = vecConvErr[i][0] / (nFilterLnCnt * nFilterColCnt);
            // Fill the traced elemets on error vector according to the input vector and filter
            for (auto j = 0; j < nFilterLnCnt; j++)
                for (auto k = 0; k < nFilterColCnt; k++)
                    vecPoolErr[nFilterLnLoc + j][nFilterColLoc + k] = dAvg;
        }
        // Max pooling
        else
        {
            // Set mark as the max value's position of the input vector
            auto nMaxLn = nFilterLnLoc, nMaxCol = nFilterColLoc;
            for (auto j = 0; j < nFilterLnCnt; j++)
                for (auto k = 0; k < nFilterColCnt; k++)
                    if (vecInput[nFilterLnLoc + j][nFilterColLoc + k] > vecInput[nMaxLn][nMaxCol])
                    {
                        nMaxLn = nFilterLnLoc + j;
                        nMaxCol = nFilterColLoc + k;
                    }
            //Assign
            vecPoolErr[nMaxLn][nMaxCol] = vecConvErr[i][0];
        }
        // Next stride
        nFilterColLoc += nFilterColCnt;
        if (nFilterColLoc == vecInput.get_column())
        {
            nFilterColLoc = 0;
            nFilterLnLoc += nFilterLnCnt;
        }
    }
    return vecPoolErr;
}
algo_queue<Matrix::matrix> PoolBackProp(algo_queue<Matrix::matrix> &mapErr, algo_queue<Matrix::matrix> &mapInput, uint64_t nFilterLnCnt, uint64_t nFilterColCnt, bool bIsAvgPool = true)
{
    algo_queue<Matrix::matrix> mapPreErr(mapErr.size());
    for(auto i=0; i<mapErr.size(); i++)
        mapPreErr[i] = PoolBackProp(mapErr[i], mapInput[i], nFilterLnCnt, nFilterColCnt, bIsAvgPool);
    return mapPreErr;
}
Matrix::matrix ConvPreErr(Matrix::matrix &vecCurrErr, Matrix::matrix &vecKernel, uint64_t nStride)
{
    auto vecPadCurrErr = Matrix::matrix::padding(vecCurrErr, vecKernel.get_line() - 1, vecKernel.get_column() - 1, nStride - 1, nStride - 1);
    auto vecRotPiRadKernel = Matrix::matrix::rotate_rect(Matrix::matrix::rotate_rect(vecKernel));
    auto vecConvPadCurrErr = GetConvVec(vecPadCurrErr, vecRotPiRadKernel.get_line(), vecRotPiRadKernel.get_column(), 1, 1);
    auto vecConvRotPiRadKernel = GetConvVec(vecRotPiRadKernel);
    // ∂L/∂x = ∂L/∂z ∂z/∂x
    auto vecConvPreErrGrad = vecConvPadCurrErr.first * vecConvRotPiRadKernel;
    // Previous error delta(i-1) = pad(delta(i)) conv rotπ(kernel)
    auto vecPreErrGrad = GetConvOutputOriginVec(vecConvPreErrGrad, vecPadCurrErr.get_line(), vecPadCurrErr.get_column(), vecRotPiRadKernel.get_line(), vecRotPiRadKernel.get_column(), 1, 1);
    return vecPreErrGrad;
}
// Multi-Channels
algo_queue<Matrix::matrix> ConvPreErr(algo_queue<Matrix::matrix> &vecCurrErr, algo_queue<algo_queue<Matrix::matrix>> &vecKernel, uint64_t nStride)
{
    if(vecCurrErr.size() == vecKernel.size())
    {
        auto nChanCnt = vecKernel[0].size();
        algo_queue<Matrix::matrix> vecPreErr(nChanCnt);
        // Add up channel's error
        for(auto i=0; i<nChanCnt; i++)
        {
            Matrix::matrix vecSglChanErrSum;
            for(auto j=0; j<vecCurrErr.size(); j++) if(vecKernel[j].size() == nChanCnt)
            {
                auto vecSglChanErr = ConvPreErr(vecCurrErr[j], vecKernel[j][i], nStride);
                if(vecSglChanErrSum.is_matrix()) vecSglChanErrSum += vecSglChanErr;
                else vecSglChanErrSum = vecSglChanErr;
            }
            else
            {
                vecPreErr.clear();
                return algo_queue<Matrix::matrix>();
            }
            vecPreErr[i] = vecSglChanErrSum;
        }
        return vecPreErr;
    }
    else return algo_queue<Matrix::matrix>();
}
Matrix::matrix KernelGrad(Matrix::matrix &vecInput, Matrix::matrix &vecCurrErr)
{
    // ∂L/∂δ ⊙ σ'(pre_signal) = ∂L/∂z
    auto vecConvInput = GetConvVec(vecInput, vecCurrErr.get_line(), vecCurrErr.get_column(), 1, 1);
    auto vecConvCurrErr = GetConvVec(vecCurrErr);
    // ∂L/∂W = ∂L/∂z ∂z/∂W
    auto vecConvKernelGrad = vecConvInput.first * vecConvCurrErr;
    auto vecKernelGrad = GetConvOutputOriginVec(vecConvKernelGrad, vecInput.get_line(), vecInput.get_column(), vecCurrErr.get_line(), vecCurrErr.get_column(), 1, 1);
    return vecKernelGrad;
}
Matrix::matrix ConvBiasGrad(algo_queue<Matrix::matrix> &vecCurrErr)
{
    auto nLnCnt = vecCurrErr[0].get_line(), nColCnt = vecCurrErr[0].get_column();
    Matrix::matrix vecConvBiasGrad(nLnCnt, nColCnt);
    for(auto i=0; i<vecCurrErr.size(); i++)
        if(vecCurrErr[i].get_line() == nLnCnt && vecCurrErr[i].get_column() == nColCnt) vecConvBiasGrad += vecCurrErr[i];
        else
        {
            vecConvBiasGrad.destroy();
            return Matrix::matrix();
        }
    return vecConvBiasGrad;
}
// U2Net
Matrix::matrix DeconvForwProp(Matrix::matrix &vecInput, Matrix::matrix &vecKernel, uint64_t nStride)
{
    auto vecPadInput = Matrix::matrix::padding(vecInput, vecKernel.get_line()-1, vecKernel.get_column()-1, nStride-1, nStride-1);
    auto vecConvPadInput = GetConvVec(vecPadInput, vecKernel.get_line(), vecKernel.get_column(), nStride, nStride);
    auto vecConvKernel = GetConvVec(vecKernel);
    auto vecConvOutput = vecConvPadInput.first * vecConvKernel;
    auto vecOutput = GetConvOutputOriginVec(vecConvOutput, vecPadInput.get_line(), vecPadInput.get_column(), vecKernel.get_line(), vecKernel.get_column(), 1, 1);
    return vecOutput;
}
algo_queue<algo_queue<Matrix::matrix>> DeconvForwProp(algo_queue<Matrix::matrix> &vecInput, algo_queue<algo_queue<Matrix::matrix>> &vecKernel, uint64_t nStride)
{
    algo_queue<algo_queue<Matrix::matrix>> vecOutput(vecKernel.size());
    for(auto i=0; i<vecKernel.size(); i++)
    {
        vecOutput[i].init(vecInput.size());
        for(auto j=0; j<vecInput.size(); j++) vecOutput[i][j] = DeconvForwProp(vecInput[j], vecKernel[i][j], nStride);
    }
    return vecOutput;
}
Matrix::matrix DeconvPreErr(Matrix::matrix &vecCurrErr, Matrix::matrix &vecKernel, uint64_t nStride)
{
    auto vecPadPreErr = ConvPreErr(vecCurrErr, vecKernel, nStride);
    auto nLnBegin = vecKernel.get_line() - 1, nColBegin = vecKernel.get_column() - 1;
    auto nItrStride = nStride - 1;
    auto nOriginPreErrLnCnt = (vecPadPreErr.get_line() - 2 * vecKernel.get_line() + 3) / 2;
    auto nOriginPreErrColCnt = (vecPadPreErr.get_column() - 2 * vecKernel.get_column() + 3) / 2;
    Matrix::matrix vecPreErr(nOriginPreErrLnCnt, nOriginPreErrColCnt);
    for(auto i=0; i<vecPreErr.get_line(); i++)
        for(auto j=0; j<vecPreErr.get_column(); j++) vecPreErr[i][j] = vecCurrErr[nLnBegin+i*nItrStride][nColBegin+j*nItrStride];
    return vecPreErr;
}
algo_queue<Matrix::matrix> DeconvPreErr(algo_queue<Matrix::matrix> &vecCurrErr, algo_queue<algo_queue<Matrix::matrix>> &vecKernel, uint64_t nStride)
{
    auto nChannCnt = vecKernel[0].size();
    algo_queue<Matrix::matrix> vecPreErr(nChannCnt);
    for(auto i=0; i<nChannCnt; i++)
    {
        Matrix::matrix vecSglChannErr;
        for(auto j=0; j<vecKernel.size(); j++)
        {
            auto vecSglErr = DeconvPreErr(vecCurrErr[j], vecKernel[j][i], nStride);
            if(vecSglChannErr.is_matrix()) vecSglChannErr += vecSglErr;
            else vecSglChannErr = vecSglErr;
        }
        vecPreErr[i] = vecSglChannErr;
    }
    return vecPreErr;
}
Matrix::matrix DeconvKernelGrad(Matrix::matrix &vecInput, Matrix::matrix &vecKernel, uint64_t nStride, Matrix::matrix &vecCurrErr)
{
    auto vecPadInput = Matrix::matrix::padding(vecInput, vecKernel.get_line()-1, vecKernel.get_column()-1, nStride-1, nStride-1); 
    auto vecConvPadInput = GetConvVec(vecPadInput, vecKernel.get_line(), vecKernel.get_column(), nStride, nStride);
    auto vecConvCurrErr = GetConvVec(vecCurrErr);
    auto vecConvGradKernel = vecConvPadInput.first * vecConvCurrErr;
    return GetConvOutputOriginVec(vecConvGradKernel, vecPadInput.get_line(), vecPadInput.get_column(), vecCurrErr.get_line(), vecCurrErr.get_column(), 1, 1);
}
algo_queue<Matrix::matrix> GradImgGray(Matrix::matrix &vecCurrErr)
{
    auto nLnCnt = vecCurrErr.get_line(),
        nColCnt = vecCurrErr.get_column();
    algo_queue<Matrix::matrix> vecErr(imgdig::RGB_CHANNEL_COUNT);
    for(auto i=0; i<nLnCnt; i++)
        for(auto j=0; j<nColCnt; j++)
        {
            if(!vecErr[imgdig::R].is_matrix()) vecErr[imgdig::R] = Matrix::matrix(nLnCnt, nColCnt);
            vecErr[imgdig::R][i][j] = imgdig::RED_GRAY_WEIGHT * vecCurrErr[i][j];
            if(!vecErr[imgdig::G].is_matrix()) vecErr[imgdig::G] = Matrix::matrix(nLnCnt, nColCnt);
            vecErr[imgdig::G][i][j] = imgdig::GREEN_GRAY_WEIGHT * vecCurrErr[i][j];
            if(!vecErr[imgdig::B].is_matrix()) vecErr[imgdig::B] = Matrix::matrix(nLnCnt, nColCnt);
            vecErr[imgdig::B][i][j] = imgdig::BLUE_GRAY_WEIGHT * vecCurrErr[i][j];
        }
    return vecErr;
}
Matrix::matrix BoolSet(Matrix::matrix &vecInput, double dThreshold)
{
    Matrix::matrix vecBool(vecInput.get_line(), vecInput.get_column());
    for(auto i=0; i<vecInput.get_line(); i++)
        for(auto j=0; j<vecInput.get_column(); j++)
            if(vecInput[i][j] < dThreshold) vecBool[i][j] = false;
            else vecBool[i][j] = true;
    return vecBool;
}
double GradThreshold(Matrix::matrix &vecCurrErr)
{
    return vecCurrErr.sum_elem();
}
Matrix::matrix BoolSetPreErr(Matrix::matrix &vecCurrErr, Matrix::matrix &vecOutput)
{
    if(vecCurrErr.shape_examin(vecOutput))
    {
        Matrix::matrix vecPreErr(vecCurrErr.get_line(), vecCurrErr.get_column());
        for(auto i=0; i<vecPreErr.get_line(); i++)
            for(auto j=0; j<vecPreErr.get_column(); j++)
                if(vecOutput[i][j]) vecPreErr[i][j] = vecCurrErr[i][j];
        return vecPreErr;
    }
    else return Matrix::matrix();
}
/* Recycle Neural Network(RNN) definition
 * Step 1 - Recycle forward propagation
 * Step 2 - Back propagation 
 */
// Return number definition
constexpr uint64_t
SIG_OUT = 0,
ACT_OUT = 1,
RE_WEIGHT = 2,
SIG_PRE_HID_LAYER = 3,
SIG_CURR_HID_LAYER = 4,
ACT_HID_LAYER = 5,
IN_WEIGHT = 6,
OUT_WEIGHT = 7,
REC_ERR = 8;

/* Whether the propagation calculation is valid
 * - Parameter
 * vecInput             [Quote] Input vector                -
 * vecReWeight          [Quote] Recycle weight              -
 * vecPreActHidLayer    [Quote] Previous activated output   -
 * vecInputWeight       [Quote] Input weight vector         -
 * vecOutputWeight      [Quote] Output weight vector        -
 * nHidLayerDim         [Quote] Hidden layer's dimension    -
 * nOutputLayerDim      [Quote] Output layer's dimension    -
 * - Return
 * [true]   Valid
 * [false]  Invalid
 */
bool IsRNNForwPropValid(Matrix::matrix &vecInput, Matrix::matrix &vecReWeight, Matrix::matrix &vecPreActHidLayer, Matrix::matrix &vecInputWeight, Matrix::matrix &vecOutputWeight, uint64_t nHidLayerDim, uint64_t nOutputLayerDim)
{
    if(vecInputWeight.get_line()==nHidLayerDim &&
    vecReWeight.get_line()==nHidLayerDim &&
    vecPreActHidLayer.get_line()==vecReWeight.get_column() &&
    vecReWeight.get_column()==vecInputWeight.get_column() &&
    vecInputWeight.get_column()==vecInput.get_line() &&
    vecOutputWeight.get_column()==nHidLayerDim &&
    vecOutputWeight.get_line()==nOutputLayerDim) return true;
    else return false;
}
/* Individual layer's calculation
 * - Parameter
 * vecInput             [Quote] Input vector                                -
 * vecReWeight          [Quote] Recycle weight                              -
 * vecPreAxctHidLayer   [Quote] Previous hidden layer's activated putput    -
 * pairInOutputWeight   [Input] I/O weight vector                           -
 * .first   Input weight vector
 * .second  Output weight vector
 * pairInOutputAdjVec   [Input] I/O adjusted vector                         -
 * .first   Input adjusted vector
 * .second  Output adjusted vector
 * funcInputAct         [Input] Input activated function                    -
 * funcOutputAct        [Input] Outputactivated function                    -
 * - Return
 * [SIG_OUT]            Signal output vector
 * [ACT_OUT]            Activated Output vector
 * [RE_WEIGHT]          Recycle weight vector cache
 * [SIG_PRE_HID_LAYER]  Previous hidden layer's propagation signal output vector
 * [SIG_CURR_HID_LAYER] Current hidden layer's signal output vector
 * [ACT_HID_LAYER]      Current hidden layer's activated output vector
 * [IN_WEIGHT]          Input weight
 * [OUT_WEIGHT]         Output weight
 */
algo_queue<Matrix::matrix> RNNForwProp(Matrix::matrix &vecInput, Matrix::matrix &vecReWeight, Matrix::matrix &vecPreActHidLayer, std::pair<Matrix::matrix, Matrix::matrix>pairInOutputWeight, std::pair<Matrix::matrix, Matrix::matrix>pairInOutputAdjVec, Matrix::matrix(*funcInputAct)(Matrix::matrix&), Matrix::matrix(*funcOutputAct)(Matrix::matrix&))
{
    algo_queue<Matrix::matrix> mapRNNProCalSeq(8);
    if(!vecReWeight.is_matrix() && !vecPreActHidLayer.is_matrix())
    {
        vecPreActHidLayer = Matrix::matrix(vecInput.get_line(), vecInput.get_column());
        vecReWeight.rand_vec(vecInput.get_line(), pairInOutputAdjVec.first.get_line());
    }
    if(!pairInOutputWeight.first.is_matrix()) pairInOutputWeight.first.rand_vec(vecInput.get_line(), pairInOutputAdjVec.first.get_line());
    if(!pairInOutputWeight.second.is_matrix()) pairInOutputWeight.second.rand_vec(pairInOutputAdjVec.first.get_line(), pairInOutputAdjVec.second.get_line());
    if(IsRNNForwPropValid(vecInput, vecReWeight, vecPreActHidLayer, pairInOutputWeight.first, pairInOutputWeight.second, pairInOutputAdjVec.first.get_line(), pairInOutputAdjVec.second.get_line()))
    {

        auto vecCurrSigHidLayer = pairInOutputWeight.first * vecInput;
        auto vecPreSigHidLayer = vecReWeight * vecPreActHidLayer;
        auto vecCurrActHidLayer = funcInputAct(vecCurrSigHidLayer + vecPreSigHidLayer + pairInOutputAdjVec.first);
        auto vecCurrSigOutputLayer = pairInOutputWeight.second * vecCurrActHidLayer;
        auto vecCurrActOutputLayer = funcOutputAct(vecCurrSigOutputLayer + pairInOutputAdjVec.second);
        // Signal&Activate Output
        mapRNNProCalSeq[SIG_OUT] = vecCurrSigOutputLayer;
        mapRNNProCalSeq[ACT_OUT] = vecCurrActOutputLayer;
        // Recycle weight
        mapRNNProCalSeq[RE_WEIGHT] = vecReWeight;
        // Signal&Activate hidden layer
        mapRNNProCalSeq[SIG_PRE_HID_LAYER] = vecPreSigHidLayer;
        mapRNNProCalSeq[SIG_CURR_HID_LAYER] = vecCurrSigHidLayer;
        mapRNNProCalSeq[ACT_HID_LAYER] = vecCurrActHidLayer;
        // I/O weight
        mapRNNProCalSeq[IN_WEIGHT] = pairInOutputWeight.first;
        mapRNNProCalSeq[OUT_WEIGHT] = pairInOutputWeight.second;
    }     
    return mapRNNProCalSeq;
}
// RNN recycle layer
struct RNN_FOR_REC
{
    // Vadication
    bool bIsRec = false;
    // Error
    Matrix::matrix vecErr;
    // Input
    Matrix::matrix vecInput;
    // Current hidden layer's signal
    Matrix::matrix vecCurrSigHidLayer;
    // Current hidden layer's activation
    Matrix::matrix vecActHidLayer;
    // Input weight
    Matrix::matrix vecInputWeight;
    // Output weight
    Matrix::matrix vecOutputWeight;
    // Signal output
    Matrix::matrix vecSigOutput;
    // Activated output
    Matrix::matrix vecActOutput;
    // Input adjustment
    Matrix::matrix vecInputAdj;
    // Output adjustment
    Matrix::matrix vecOutputAdj;
    RNN_FOR_REC()
    {
        bIsRec = false;
    }
    ~RNN_FOR_REC(){}
    RNN_FOR_REC(Matrix::matrix &vecErr, Matrix::matrix &vecInput, Matrix::matrix &vecSigHidLayer, Matrix::matrix &vecActHidLayer, Matrix::matrix &vecInputWeight, Matrix::matrix &vecOutputWeight, Matrix::matrix &vecCurrSigHidLayer, Matrix::matrix &vecActOutput, Matrix::matrix &vecInputAdj, Matrix::matrix &vecOutputAdj)
    {
        this->vecErr = vecErr;
        this->vecInput = vecInput;
        this->vecCurrSigHidLayer = vecCurrSigHidLayer;
        this->vecActHidLayer = vecActHidLayer;
        this->vecOutputWeight = vecOutputWeight;
        this->vecSigOutput = vecSigOutput;
        this->vecActOutput = vecActOutput;
        this->vecInputAdj = vecInputAdj;
        this->vecOutputAdj = vecOutputAdj;
        bIsRec = true;
    }
    void operator=(RNN_FOR_REC &RNN_FOR_REC_SRC)
    {
        vecErr = RNN_FOR_REC_SRC.vecErr;
        vecInput = RNN_FOR_REC_SRC.vecInput;
        vecCurrSigHidLayer = RNN_FOR_REC_SRC.vecCurrSigHidLayer;
        vecActHidLayer = RNN_FOR_REC_SRC.vecActHidLayer;
        vecOutputWeight = RNN_FOR_REC_SRC.vecOutputWeight;
        vecSigOutput = RNN_FOR_REC_SRC.vecSigOutput;
        vecActOutput = RNN_FOR_REC_SRC.vecActOutput;
        vecInputAdj = RNN_FOR_REC_SRC.vecInputAdj;
        vecOutputAdj = RNN_FOR_REC_SRC.vecOutputAdj;
        bIsRec = true;
    }
};
/* Back propagation calculation
 * Input the RNN back layer
 * - Parameter
 * vecReWeightDelta [Input/Output]  Accumulated recycle weight error            -
 * vecPreBackErr    [Quote]         Next layer's error which is propagated back -
 * vecReWeight      [Quote]         Recycle weight                              -
 * CurrRNNBackRec   [Quote]         Current RNN's information set               -
 * PreRNNBackRec    [Quote]         Previous RNN's information  set             -
 * funcInputDv      [Input]         Input activated function derivative         -
 * funcOutputDv     [Input]         Output  activated function derivative       -
 * dTrainRate       [Input]         Train rate                                  0.8
 * - Return
 * [SIG_OUT]            Signal output vector
 * [ACT_OUT]            Activated Output vector
 * [RE_WEIGHT]          Recycle weight vector cache
 * [SIG_PRE_HID_LAYER]  Previous hidden layer's propagation signal output vector
 * [SIG_CURR_HID_LAYER] Current hidden layer's signal output vector
 * [ACT_HID_LAYER]      Current hidden layer's activated output vector
 * [IN_WEIGHT]          Input weight
 * [OUT_WEIGHT]         Output weight
 * [REC_ERR]            Recycle error
 */
algo_queue<Matrix::matrix> RNNBackProp(Matrix::matrix &vecReWeightDelta, Matrix::matrix &vecPreBackErr, Matrix::matrix &vecReWeight, RNN_FOR_REC &CurrRNNBackRec, RNN_FOR_REC &PreRNNBackRec, Matrix::matrix(*funcInputDv)(Matrix::matrix&), Matrix::matrix(*funcOutputDv)(Matrix::matrix&), double dTrainRate = 8e-1)
{
    algo_queue<Matrix::matrix> mapBackProLayerSeq(9);
    mapBackProLayerSeq[SIG_OUT] = CurrRNNBackRec.vecSigOutput;
    mapBackProLayerSeq[ACT_OUT] = CurrRNNBackRec.vecActOutput;
    mapBackProLayerSeq[RE_WEIGHT] = vecReWeight;
    mapBackProLayerSeq[SIG_PRE_HID_LAYER] = PreRNNBackRec.vecCurrSigHidLayer;
    mapBackProLayerSeq[SIG_CURR_HID_LAYER] = CurrRNNBackRec.vecCurrSigHidLayer;
    mapBackProLayerSeq[ACT_HID_LAYER] = CurrRNNBackRec.vecActHidLayer;
    // Current round's output error
    auto vecCurrOutputErr = CurrRNNBackRec.vecErr;
    // Current round's input error sum
    auto vecHidErr = Matrix::matrix::transposition(CurrRNNBackRec.vecOutputWeight) * vecCurrOutputErr;
    // The hidden activated output will be used to generate current layer's output and next join the next round's hidden layer's calculation according to forward propagation.
    if(vecPreBackErr.is_matrix()) vecHidErr += vecPreBackErr;
    // Hidden signal degenerated error
    Matrix::matrix vecDeGenSigInput;
    if(PreRNNBackRec.bIsRec) vecDeGenSigInput = funcInputDv(CurrRNNBackRec.vecCurrSigHidLayer + PreRNNBackRec.vecActHidLayer) - CurrRNNBackRec.vecInputAdj;
    else vecDeGenSigInput = funcInputDv(CurrRNNBackRec.vecCurrSigHidLayer) - CurrRNNBackRec.vecInputAdj;
    // Previous round's error and derivative
    Matrix::matrix vecPreHidErr(vecHidErr.get_line(), 1);
    Matrix::matrix vecPreHidDeGenSigOutput(vecHidErr.get_line(), 1);
    // Current round's error and derivative
    Matrix::matrix vecCurrHidErr(vecHidErr.get_line(), 1);
    Matrix::matrix vecCurreHidDeGenSigOutput(vecHidErr.get_line(), 1);
    // Recycle signal vector
    Matrix::matrix vecReSigOutput;
    if(PreRNNBackRec.bIsRec) vecReSigOutput = vecReWeight * PreRNNBackRec.vecActHidLayer;
    // Divide the error
    if(!CurrRNNBackRec.vecInput.is_matrix() || CurrRNNBackRec.vecInput.is_O_matrix())
    {
        // If the input vector is discarded
        vecPreHidErr = vecHidErr;
        vecPreHidDeGenSigOutput = vecDeGenSigInput;
    }
    else
    {
        for(auto j=0; j<vecHidErr.get_line(); j++) if(PreRNNBackRec.bIsRec)
        {
            auto dAlloc = CurrRNNBackRec.vecCurrSigHidLayer[j][0] + vecReSigOutput[j][0];
            vecPreHidErr[j][0] = vecHidErr[j][0] * (vecReSigOutput[j][0] / dAlloc);
            vecCurrHidErr[j][0] = vecHidErr[j][0] * (CurrRNNBackRec.vecCurrSigHidLayer[j][0] / dAlloc);
            vecPreHidDeGenSigOutput[j][0] = vecDeGenSigInput[j][0] * (vecReSigOutput[j][0] / dAlloc);
            vecCurreHidDeGenSigOutput[j][0] = vecDeGenSigInput[j][0] * (CurrRNNBackRec.vecCurrSigHidLayer[j][0] / dAlloc);
        }
        else
        {
            vecCurrHidErr[j][0] = vecHidErr[j][0];
            vecCurreHidDeGenSigOutput[j][0] = vecDeGenSigInput[j][0];
        }
    }
    // Output delta
    auto vecOutputTrainErr = vecCurrOutputErr * dTrainRate;
    auto vecDeGenSigOutput = funcOutputDv(CurrRNNBackRec.vecSigOutput) - CurrRNNBackRec.vecOutputAdj;
    for(auto j=0; j<vecOutputTrainErr.get_line(); j++) vecOutputTrainErr[j][0] *= vecDeGenSigOutput[j][0];
    auto vecOutputDelta = vecOutputTrainErr * Matrix::matrix::transposition(CurrRNNBackRec.vecActHidLayer);
    // Input delta
    Matrix::matrix vecInputDelta;
    if(!CurrRNNBackRec.vecInput.is_matrix() || CurrRNNBackRec.vecInput.is_O_matrix())
        vecInputDelta = Matrix::matrix(CurrRNNBackRec.vecInputWeight.get_line(), CurrRNNBackRec.vecInputWeight.get_column());
    else 
    {
        auto vecInputTrainErr = vecCurrHidErr * dTrainRate;
        for(auto j=0; j<vecInputTrainErr.get_line(); j++)
            vecInputTrainErr[j][0] *= vecCurreHidDeGenSigOutput[j][0];
        vecInputDelta = vecInputTrainErr * Matrix::matrix::transposition(CurrRNNBackRec.vecInput);
    }
    auto vecNewInputWeight = CurrRNNBackRec.vecInputWeight + vecInputDelta;
    auto vecNewOutputWeight = CurrRNNBackRec.vecOutputWeight + vecOutputDelta;
    mapBackProLayerSeq[IN_WEIGHT] = vecNewInputWeight;
    mapBackProLayerSeq[OUT_WEIGHT] = vecNewOutputWeight;
    /* Recycle delta
     * This value will be accumulated if the current round is not the first.
     */
    if(PreRNNBackRec.bIsRec)
    {
        vecPreBackErr = Matrix::matrix::transposition(vecReWeight) * vecPreHidErr;
        mapBackProLayerSeq[REC_ERR] = vecPreBackErr;
        auto vecPreHidTrainErr = vecPreHidErr * dTrainRate;
        for(auto j=0; j<vecPreHidTrainErr.get_line(); j++)
            vecPreHidTrainErr[j][0] *= vecPreHidDeGenSigOutput[j][0];
        if(vecReWeightDelta.is_matrix())
        // Add up all errors of each recycle layers after splitting the input error
            vecReWeightDelta += vecPreHidTrainErr * Matrix::matrix::transposition(PreRNNBackRec.vecActHidLayer);
        else vecReWeightDelta = vecPreHidTrainErr * Matrix::matrix::transposition(PreRNNBackRec.vecActHidLayer);
    }
    return mapBackProLayerSeq;
}
/* Get back propagation Parameter
 * - Parameter
 * vecReWeight      [Quote] Recycle weight                          -
 * mapRNNBackRecSeq [Input] RNN information set                     -
 * funcInputDv      [Input] Input activated function detrivative    -
 * funcOutputDv     [Input] Output activated function detrivative   -
 * dTrainRate       [Input] Train rate                              0.8
 * - Return
 * .first               Updated recycle vector
 * .second              Updated I/O vector weight
 * .second[n].first     Updated layer n's input weight vector
 * .second[n].second    Updated layer n's output weight vector
 */
std::pair<Matrix::matrix, algo_queue<std::pair<Matrix::matrix, Matrix::matrix>>> GetRNNBackPropSeq(Matrix::matrix &vecReWeight, algo_queue<RNN_FOR_REC>mapRNNBackRecSeq, Matrix::matrix(funcInputDv)(Matrix::matrix&), Matrix::matrix(*funcOutputDv)(Matrix::matrix&), double dTrainRate = 8e-1)
{
    Matrix::matrix vecReWeightDelta;
    Matrix::matrix vecPreBackErr;
    algo_queue<std::pair<Matrix::matrix, Matrix::matrix>> mapNewWeightSeq(mapRNNBackRecSeq.size());
    for(auto i=mapRNNBackRecSeq.size(); i>0; i--)
    {
        RNN_FOR_REC PreRNNBackRec;
        if(i - 2 >= 0) PreRNNBackRec = mapRNNBackRecSeq[i - 2];
        auto mapRNNBPCalSeq = RNNBackProp(vecReWeightDelta, vecPreBackErr, vecReWeight,
        mapRNNBackRecSeq[i - 1], PreRNNBackRec, funcInputDv, funcOutputDv, dTrainRate);
        mapNewWeightSeq[i - 1] = std::make_pair(mapRNNBPCalSeq[IN_WEIGHT], mapRNNBPCalSeq[OUT_WEIGHT]);
    }
    return std::make_pair(vecReWeight + vecReWeightDelta, mapNewWeightSeq);
}
NEUNET_END// namespace neunet
