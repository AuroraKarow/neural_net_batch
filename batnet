/* This is a C++ 17 head file
 * Date     2021-04-27
 * Author   Liao 
 */
#include "neunet"
#define BATNET_BEGIN namespace batnet {
#define BATNET_END }

// namespace begin
BATNET_BEGIN

// Fully connection network
algo_queue<Matrix::matrix> ForwProp(algo_queue<Matrix::matrix> &batInput, Matrix::matrix &vecWeight)
{
    algo_queue<Matrix::matrix> batSigOutput(batInput.size());
    for(auto i=0; i<batInput.size(); i++) batSigOutput[i] = neunet::ForwProp(batInput[i], vecWeight);
    return batSigOutput;
}
algo_queue<Matrix::matrix> DerivativeErr(algo_queue<Matrix::matrix> &batCurrErr, algo_queue<Matrix::matrix> &batInput,Matrix::matrix(*funcActDv)(Matrix::matrix&))
{
    algo_queue<Matrix::matrix> batPreErr(batCurrErr.size());
    for(auto i=0; i<batCurrErr.size(); i++) batPreErr[i] = funcActDv(batInput[i]).elem_mult(batCurrErr[i]);
    return batPreErr;
}
algo_queue<Matrix::matrix> PreErr(algo_queue<Matrix::matrix> &batCurrErr, Matrix::matrix &vecWeight)
{
    algo_queue<Matrix::matrix> batPreErr(batCurrErr.size());
    for(auto i=0; i<batCurrErr.size(); i++) batPreErr[i] = neunet::PreErr(batCurrErr[i], vecWeight);
    return batPreErr;
}
Matrix::matrix WeightGrad(algo_queue<Matrix::matrix> &batCurrErr, algo_queue<Matrix::matrix> &batInput)
{
    Matrix::matrix vecWeightGrad;
    if(batCurrErr.size() == batInput.size()) for(auto i=0; i<batInput.size(); i++)
    {
        auto vecSglWeightGrad = neunet::WeightGrad(batCurrErr[i], batInput[i]);
        if(vecWeightGrad.is_matrix()) vecWeightGrad += vecSglWeightGrad;
        else vecWeightGrad = vecSglWeightGrad;
    }
    return vecWeightGrad;
}
Matrix::matrix InitWeight(uint64_t nInputDim, uint64_t nOutputDim, double dHeadBond = 1, double dTailBond = 1)
{
    Matrix::matrix batInitWeight(nOutputDim, nInputDim);
    batInitWeight.rand_vec(nOutputDim, nInputDim, dHeadBond, dTailBond);
    return batInitWeight;
}
algo_queue<Matrix::matrix> Activate(algo_queue<Matrix::matrix> &batSignal, Matrix::matrix(*funcAct)(Matrix::matrix&))
{
    algo_queue<Matrix::matrix> batActOutput(batSignal.size());
    for(auto i=0; i<batSignal.size(); i++) batActOutput[i] = funcAct(batSignal[i]);
    return batActOutput;
}
// Gauss connection
algo_queue<Matrix::matrix> GaussConnForwProp(algo_queue<Matrix::matrix> &batInput, Matrix::matrix &vecWeight)
{
    algo_queue<Matrix::matrix> batGaussConnOutput(batInput.size());
    for(auto i=0; i<batInput.size(); i++) batGaussConnOutput[i] = neunet::GaussConnForwProp(batInput[i], vecWeight);
    return batGaussConnOutput;
}
algo_queue<Matrix::matrix> GaussConnPreErr(algo_queue<Matrix::matrix> &batInput, algo_queue<Matrix::matrix> &batOutput, algo_queue<Matrix::matrix> &batOrigin, Matrix::matrix &vecWeight, double dLearnRate)
{
    algo_queue<Matrix::matrix> batGaussConnPreErr;
    if(batOutput.size() == batOrigin.size() && batOrigin.size() == batInput.size())
    {
        batGaussConnPreErr.init(batInput.size());
        Matrix::matrix vecWeightGrad(vecWeight.get_line(), vecWeight.get_column());
        for(auto i=0; i<batInput.size(); i++)
        {
            auto pairSglErrWeightGrad = neunet::GaussConnPreErr(batInput[i], batOutput[i], batOrigin[i], vecWeight);
            batGaussConnPreErr[i] = pairSglErrWeightGrad.first;
            vecWeightGrad += pairSglErrWeightGrad.second;
        }
        vecWeight -= vecWeightGrad * dLearnRate;
        return batGaussConnPreErr;
    }
    else return batGaussConnPreErr;
}
// CNN's FCN I/O pre-process
algo_queue<Matrix::matrix> CNNFCNIO(algo_queue<algo_queue<Matrix::matrix>> &batAtomOutput)
{
    algo_queue<Matrix::matrix> batFCNInput(batAtomOutput.size());
    for(auto i=0; i<batAtomOutput.size(); i++) batFCNInput[i] = neunet::CNNFCNIO(batAtomOutput[i]);
    return batFCNInput;
}
algo_queue<algo_queue<Matrix::matrix>> CNNFCNIO(algo_queue<Matrix::matrix> &batCurrErr)
{
    algo_queue<algo_queue<Matrix::matrix>> batCurrPreErr(batCurrErr.size());
    for(auto i=0; i<batCurrErr.size(); i++) batCurrPreErr[i] = neunet::CNNFCNIO(batCurrErr[i]);
    return batCurrPreErr;
}
// Convolution
algo_queue<algo_queue<algo_queue<Matrix::matrix>>> ConvForwProp(algo_queue<algo_queue<Matrix::matrix>> &batInput, algo_queue<algo_queue<Matrix::matrix>> &vecKernel, uint64_t nStride)
{
    algo_queue<algo_queue<algo_queue<Matrix::matrix>>> batSigOutput(batInput.size());
    // Batch count
    for(auto i=0; i<batInput.size(); i++)
    {
        // Kernel count
        batSigOutput[i].init(vecKernel.size());
        for(auto j=0; j<vecKernel.size(); j++)
            batSigOutput[i][j] = neunet::ConvForwProp(batInput[i], vecKernel[j], nStride);
    }
    return batSigOutput;
}
algo_queue<algo_queue<Matrix::matrix>> MergeChannel(algo_queue<algo_queue<algo_queue<Matrix::matrix>>> &batSigOutput)
{
    algo_queue<algo_queue<Matrix::matrix>> MergedSigOutput(batSigOutput.size());
    for(auto i=0; i<batSigOutput.size(); i++)
    {   
        MergedSigOutput[i].init(batSigOutput[i].size());
        for(auto j=0; j<batSigOutput[i].size(); j++) MergedSigOutput[i][j] = neunet::MergeChannel(batSigOutput[i][j]);
    }
    return MergedSigOutput;
}
algo_queue<algo_queue<Matrix::matrix>> ConvDerivativeErr(algo_queue<algo_queue<Matrix::matrix>> &batCurrErr, algo_queue<algo_queue<Matrix::matrix>> &batInput, Matrix::matrix(*funcActDv)(Matrix::matrix&))
{
    algo_queue<algo_queue<Matrix::matrix>> batPreErr(batCurrErr.size());
    for(auto i=0; i<batCurrErr.size(); i++)
    {
        batPreErr[i].init(batCurrErr[i].size());
        for(auto j=0; j<batCurrErr[i].size(); j++) batPreErr[i][j] = funcActDv(batInput[i][j]).elem_mult(batCurrErr[i][j]);
    }
    return batPreErr;
}
algo_queue<algo_queue<Matrix::matrix>> ConvPreErr(algo_queue<algo_queue<Matrix::matrix>> &batCurrErr, algo_queue<algo_queue<Matrix::matrix>> &vecKernel, uint64_t nStride)
{
    algo_queue<algo_queue<Matrix::matrix>> batConvPreErr(batCurrErr.size());
    for(auto i=0; i<batCurrErr.size(); i++)if(batCurrErr[i].size() == vecKernel.size())
        batConvPreErr[i] = neunet::ConvPreErr(batCurrErr[i], vecKernel, nStride);
    else
    {
        batConvPreErr.clear();
        break;
    }
    return batConvPreErr;
}
algo_queue<algo_queue<Matrix::matrix>> KernelGrad(algo_queue<algo_queue<Matrix::matrix>> &batInput, algo_queue<algo_queue<Matrix::matrix>> &batCurrErr)
{
    algo_queue<algo_queue<Matrix::matrix>> batKernelGrad;
    if(batCurrErr.size() == batInput.size())
    {
        // Kernel count
        auto nKernelCnt = batCurrErr[0].size();
        batKernelGrad.init(nKernelCnt);
        // Channel count
        auto nChannCnt = batInput[0].size();
        for(auto i=0; i<nKernelCnt; i++)
        {
            // Channel
            batKernelGrad[i].init(nChannCnt);
            for(auto j=0; j<nChannCnt; j++)
            {
                // Add up all of batch's gradient
                for(auto k=0; k<batInput.size(); k++) if(batInput[k].size() == nChannCnt && batCurrErr[k].size() == nKernelCnt)
                {
                    if(!batCurrErr[k][i].is_matrix())
                        auto test = 0;
                    auto vecSglKernelGrad = neunet::KernelGrad(batInput[k][j], batCurrErr[k][i]);
                    if(batKernelGrad[i][j].is_matrix()) batKernelGrad[i][j] += vecSglKernelGrad;
                    else batKernelGrad[i][j] = vecSglKernelGrad;
                }
                else
                {
                    batKernelGrad.clear();
                    return algo_queue<algo_queue<Matrix::matrix>>();
                }
            }
        }
    }
    return batKernelGrad; 
}
algo_queue<algo_queue<Matrix::matrix>> InitKernel(uint64_t nAmt, uint64_t nChanCnt, uint64_t nLnSize, uint64_t nColSize, double dHead = 1, double dRear = 1)
{
    auto nSeedCnt = nAmt * nChanCnt * nLnSize *nColSize;
    algo_queue<algo_queue<Matrix::matrix>> mapKernels(nAmt);
    for(auto i=0; i<nAmt; i++)
    {
        mapKernels[i].init(nChanCnt);
        for(auto j=0; j<nChanCnt; j++) mapKernels[i][j].rand_vec(nLnSize, nColSize, dHead, dRear);
    }
    return mapKernels;
}
algo_queue<algo_queue<Matrix::matrix>> Activate(algo_queue<algo_queue<Matrix::matrix>> &batSignal, Matrix::matrix(*funcAct)(Matrix::matrix&))
{
    algo_queue<algo_queue<Matrix::matrix>> batActOutput(batSignal.size());
    for(auto i=0; i<batSignal.size(); i++)
    {
        batActOutput[i].init(batSignal[i].size());
        for(auto j=0; j<batSignal[i].size(); j++) batActOutput[i][j] = funcAct(batSignal[i][j]);
    }
    return batActOutput;
}
bool UpdateKernel(algo_queue<algo_queue<Matrix::matrix>> &batPreKernel, algo_queue<algo_queue<Matrix::matrix>> &batKernelGrad, double dLearnRate)
{
    if(batPreKernel.size() == batKernelGrad.size())
    {
        for(auto i=0; i<batPreKernel.size(); i++)
            if(batPreKernel[i].size() == batKernelGrad[i].size())
                for(auto j=0; j<batPreKernel[i].size(); j++)
                    batPreKernel[i][j] -= batKernelGrad[i][j] * dLearnRate;
        return true;
    }
    else return false;
}
// Pooling
algo_queue<algo_queue<Matrix::matrix>> PoolForwProp(algo_queue<algo_queue<Matrix::matrix>> &batInput, uint64_t nFilterLnCnt, uint64_t nFilterColCnt, bool bAvgPool = true)
{
    algo_queue<algo_queue<Matrix::matrix>> batPoolOutput(batInput.size());
    for(auto i=0; i<batInput.size(); i++)
        batPoolOutput[i] = neunet::PoolForwProp(batInput[i], nFilterLnCnt, nFilterColCnt, bAvgPool);
    return batPoolOutput;
}
algo_queue<algo_queue<Matrix::matrix>> PoolPreErr(algo_queue<algo_queue<Matrix::matrix>> &batCurrErr, algo_queue<algo_queue<Matrix::matrix>> &batInput, uint64_t nFilterLnCnt, uint64_t nFilterColCnt, bool bAvgPool = true)
{
    algo_queue<algo_queue<Matrix::matrix>> batPoolPreErr;
    if(batCurrErr.size() == batInput.size())
    {
        batPoolPreErr.init(batCurrErr.size());
        for(auto i=0; i<batCurrErr.size(); i++) batPoolPreErr[i] = neunet::PoolBackProp(batCurrErr[i], batInput[i], nFilterLnCnt, nFilterColCnt, bAvgPool);
        return batPoolPreErr;
    }
    return batPoolPreErr;
}
// Bias
Matrix::matrix InitBias(uint64_t nLnCnt, uint64_t nColCnt, double dHeadBond = 1, double dTailBond = 1)
{
    Matrix::matrix vecBias(nLnCnt, nColCnt);
    vecBias.rand_vec(vecBias.get_line(), vecBias.get_column(), dHeadBond, dTailBond);
    return vecBias;
}
Matrix::matrix BiasGrad(algo_queue<Matrix::matrix> &vecCurrErr, algo_queue<Matrix::matrix> &batSigOutput, Matrix::matrix(*funcActDv)(Matrix::matrix&))
{
    auto nLnCnt = vecCurrErr[0].get_line(), nColCnt = vecCurrErr[0].get_column();
    Matrix::matrix vecBiasGrad(nLnCnt, nColCnt);
    for(auto i=0; i<vecCurrErr.size(); i++)
        if(vecCurrErr[i].get_line() == nLnCnt && vecCurrErr[i].get_column() == nColCnt)
            vecBiasGrad += Matrix::matrix::elem_mult(vecCurrErr[i], funcActDv(batSigOutput[i]));
        else
        {
            vecBiasGrad.destroy();
            return Matrix::matrix();
        }
    return vecBiasGrad;
}
Matrix::matrix ConvBiasGrad(algo_queue<algo_queue<Matrix::matrix>> &vecCurrErr)
{
    Matrix::matrix batConvBiasGrad;
    for(auto i=0; i<vecCurrErr.size(); i++) for(auto j=0; j<vecCurrErr[i].size(); j++)
    {
        auto vecSglConvBiasGrad = neunet::ConvBiasGrad(vecCurrErr[i]);
        if(batConvBiasGrad.is_matrix()) batConvBiasGrad += vecSglConvBiasGrad;
        else batConvBiasGrad = vecSglConvBiasGrad;
    }
    return batConvBiasGrad;
}
// Batch normalization
Matrix::matrix DivisorZeroOpt(Matrix::matrix vecDivisor, double dEpsilon)
{
    Matrix::matrix vecCpy(vecDivisor);
    for(auto i=0; i<vecCpy.get_line(); i++)
        for(auto j=0; j<vecCpy.get_column(); j++)
            if(vecCpy[i][j] == 0) vecCpy[i][j] = dEpsilon;
    return vecCpy;
}
struct BN
{
    Matrix::matrix vecMiuBeta;
    Matrix::matrix batSigmaSqr;
    algo_queue<Matrix::matrix> batNormOutput;
    algo_queue<Matrix::matrix> batScaleShift;
};
BN BatchNormalizationForwProp(algo_queue<Matrix::matrix> &batInput, double dBeta = 0, double dGamma = 1, double dEpsilon = 1e-10)
{
    BN BNOutput;
    auto nBatchSize = batInput.size();
    auto nLnCnt = batInput[0].get_line(),
        nColCnt = batInput[0].get_column();
    BNOutput.vecMiuBeta = Matrix::matrix(nLnCnt, nColCnt);
    BNOutput.batSigmaSqr = Matrix::matrix(nLnCnt, nColCnt);
    // Average
    for(auto i=0; i<nBatchSize; i++) BNOutput.vecMiuBeta += batInput[i];
    BNOutput.vecMiuBeta.elem_division(nBatchSize);
    // Variance
    for(auto i=0; i<nBatchSize; i++)
    {
        auto vecDist = batInput[i] - BNOutput.vecMiuBeta;
        vecDist.elem_power(2);
        BNOutput.batSigmaSqr += vecDist;
    }
    BNOutput.batSigmaSqr.elem_division(nBatchSize);
    // Normalize
    BNOutput.batNormOutput.init(nBatchSize);
    BNOutput.batScaleShift.init(nBatchSize);
    for(auto i=0; i<nBatchSize; i++)
    {
        // (input - μΒ) / √(σ^2 + ε)
        BNOutput.batNormOutput[i] = (batInput[i] - BNOutput.vecMiuBeta).elem_division((DivisorZeroOpt(BNOutput.batSigmaSqr, dEpsilon)).elem_power(0.5));
        // γ input + β
        BNOutput.batScaleShift[i] = BNOutput.batNormOutput[i] * dGamma + dBeta;
    }
    return BNOutput;
}
algo_queue<Matrix::matrix> BatchNormalizationPreErr(BN &batOutput, algo_queue<Matrix::matrix> &batInput, algo_queue<Matrix::matrix> &batCurrErr, double dGamma, double dEpsilon = 1e-10)
{
    auto nBatchSize = batCurrErr.size();
    auto nLnCnt = batCurrErr[0].get_line(), nColCnt = batCurrErr[0].get_column();
    // ∂L/∂ bar x
    for(auto i=0; i<nBatchSize; i++) batCurrErr[i] *= dGamma;
    // ∂L/∂σ^2 = (∂L/∂ bar x)(∂ bar x/∂σ^2)
    Matrix::matrix vecGradSigmaSqr(nLnCnt, nColCnt);
    auto vecCpySigmaSqr = batOutput.batSigmaSqr;
    auto vecDiscDistribute = DivisorZeroOpt(batOutput.batSigmaSqr, dEpsilon);;
    auto vecGradDiscDistribute = Matrix::matrix::elem_power(vecDiscDistribute, (-1.5)) * (-0.5);
    for(auto i=0; i<nBatchSize; i++)
    {
        auto vecDistance = Matrix::matrix::elem_mult(batCurrErr[i], (batInput[i]-batOutput.vecMiuBeta));
        vecGradSigmaSqr += Matrix::matrix::elem_mult(vecDistance, vecGradDiscDistribute);
    }
    // ∂L/∂μΒ = (∂L/∂ bar x)(∂ bar x/∂μΒ)
    Matrix::matrix vecGradDistribute(nLnCnt, nColCnt);
    Matrix::matrix vecGradDistance(nLnCnt, nColCnt);
    // -(σ^2 + ε)^(-0.5)
    auto vecDistribute = Matrix::matrix::elem_power(vecDiscDistribute, (-0.5));
    for(auto i=0; i<nBatchSize; i++)
    {
        vecGradDistribute += Matrix::matrix::elem_division(batCurrErr[i], vecDistribute) * (-1);
        vecGradDistance += (batInput[i] - batOutput.vecMiuBeta) * (-2);
    }
    vecGradDistance.elem_division(nBatchSize);
    vecGradDistance.elem_mult(vecGradSigmaSqr);
    auto vecGradMiuBeta = vecGradDistribute + vecGradDistance;
    // ∂L/∂ input = (∂L/∂ bar x)(∂ bar x/input)
    algo_queue<Matrix::matrix> batPreErr(nBatchSize);
    auto vecNormDistance = Matrix::matrix::elem_division(vecGradMiuBeta, nBatchSize);
    for(auto i=0; i<nBatchSize; i++)
    {
        auto vecNormBarX = Matrix::matrix::elem_division(batCurrErr[i], vecDistribute);
        auto vecNormDistribute = Matrix::matrix::elem_mult(vecGradSigmaSqr, Matrix::matrix::elem_division((batInput[i]-batOutput.vecMiuBeta)*2, nBatchSize));
        batPreErr[i] = vecNormBarX + vecNormDistribute + vecNormDistance;
    }
    return batPreErr;
}
double BNScaleGrad(algo_queue<Matrix::matrix> &batCurrErr, BN &batOutput)
{
    auto nLnCnt = batCurrErr[0].get_line(),
        nColCnt = batCurrErr[0].get_column();
    Matrix::matrix vecGradGamma(nLnCnt, nColCnt);
    for(auto i=0; i<batCurrErr.size(); i++) vecGradGamma += Matrix::matrix::elem_mult(batCurrErr[i], batOutput.batNormOutput[i]);
    return vecGradGamma.sum_elem();
}
double BNShiftGrad(algo_queue<Matrix::matrix> &batCurrErr)
{
    auto nLnCnt = batCurrErr[0].get_line(),
        nColCnt = batCurrErr[0].get_column();
    Matrix::matrix vecGradBeta(nLnCnt, nColCnt);
    for(auto i=0; i<batCurrErr.size(); i++) vecGradBeta += batCurrErr[i];
    return vecGradBeta.sum_elem();
}
struct CNNBN
{
    algo_queue<Matrix::matrix> batMiuBeta;
    algo_queue<Matrix::matrix> batSigmaSqr;
    algo_queue<algo_queue<Matrix::matrix>> batNormOutput;
    algo_queue<algo_queue<Matrix::matrix>> batScaleShift;
};
algo_queue<double> InitScaleBatch(uint64_t nChannCnt)
{
    algo_queue<double> batGamma(nChannCnt);
    for(auto i=0; i<nChannCnt; i++) batGamma[i] = 1;
    return batGamma;
}
algo_queue<double> InitShiftBatch(uint64_t nChannCnt)
{
    algo_queue<double> batBeta(nChannCnt);
    for(auto i=0; i<nChannCnt; i++) batBeta[i] = 0;
    return batBeta;
}
CNNBN ConvBatchNormalizationForwProp(algo_queue<algo_queue<Matrix::matrix>> &batInput,  algo_queue<double> &batBeta, algo_queue<double> &batGamma, double dEpsilon = 1e-5)
{
    CNNBN BNOutput;
    auto nBatchSize = batInput.size(),
        nChannCnt = batInput[0].size();
    auto nLnCnt = batInput[0][0].get_line(),
        nColCnt = batInput[0][0].get_column();
    // Average
    BNOutput.batMiuBeta.init(nChannCnt);
    // Variance
    BNOutput.batSigmaSqr.init(nChannCnt);
    for(auto i=0; i<nChannCnt; i++)
    {
        Matrix::matrix vecChannMiuBeta(nLnCnt, nColCnt);
        Matrix::matrix vecChannSigmaSqr(nLnCnt, nColCnt);
        // Σ input
        for(auto j=0; j<nBatchSize; j++) vecChannMiuBeta += batInput[j][i];
        // Σ/m
        vecChannMiuBeta.elem_division(nBatchSize);
        // Σ (input-avg)^2
        for(auto j=0; j<nBatchSize; j++)
        {
            auto vecDist = batInput[j][i] - vecChannMiuBeta;
            vecDist.elem_power(2);
            vecChannSigmaSqr += vecDist;
        }
        // Σ^2/m
        vecChannSigmaSqr.elem_division(nBatchSize);
        BNOutput.batMiuBeta[i] = vecChannMiuBeta;
        BNOutput.batSigmaSqr[i] = vecChannSigmaSqr;
    }
    // normalize
    BNOutput.batNormOutput.init(nBatchSize);
    BNOutput.batScaleShift.init(nBatchSize);
    for(auto i=0; i<nBatchSize; i++)
    {
        BNOutput.batNormOutput[i].init(nChannCnt);
        BNOutput.batScaleShift[i].init(nChannCnt);
    }
    for(auto i=0; i<nChannCnt; i++)
    {
        for(auto j=0; j<nBatchSize; j++)
        {
            // input - μΒ
            auto vecDist = batInput[j][i] - BNOutput.batMiuBeta[i];
            // σ^2 + ε
            auto vecSigma = DivisorZeroOpt(BNOutput.batSigmaSqr[i], dEpsilon);
            // √(σ^2 + ε)
            vecSigma.elem_power(0.5);
            // (input - μΒ) / √(σ^2 + ε)
            vecDist.elem_division(vecSigma);
            BNOutput.batNormOutput[j][i] = vecDist;
            BNOutput.batScaleShift[j][i] = vecDist * batGamma[i] + batBeta[i];
        }
    }
    return BNOutput;
}
algo_queue<algo_queue<Matrix::matrix>> ConvBatchNormalizationPreErr(CNNBN &batOutput, algo_queue<algo_queue<Matrix::matrix>> &batInput, algo_queue<algo_queue<Matrix::matrix>> &batCurrErr, algo_queue<double> &batGamma, double dEpsilon = 1e-10)
{
    auto nChannCnt = batCurrErr[0].size(),
        nBatchSize = batCurrErr.size();
    auto nLnCnt = batCurrErr[0][0].get_line(),
        nColCnt = batCurrErr[0][0].get_column();
    // ∂L/∂ bar x
    // Batch
    for(auto i=0; i<batCurrErr.size(); i++)
        // Channel(dimension)
        for(auto j=0; j<batCurrErr[i].size(); j++) batCurrErr[i][j] *= batGamma[j];
    // ∂L/∂σ^2
    algo_queue<Matrix::matrix> batGradSigmaSqr(nChannCnt);
    for(auto i=0; i<nChannCnt; i++)
    {
        Matrix::matrix vecGradSigmaSqr(nLnCnt, nColCnt);
        // -((σ^2 + ε) ^ (-1.5)) / 2
        auto vecSigmaSqrFact = DivisorZeroOpt(batOutput.batSigmaSqr[i], dEpsilon);
        vecSigmaSqrFact.elem_power(-1.5);
        vecSigmaSqrFact *= (-0.5);
        for(auto j=0; j<nBatchSize; j++)
        {
            // input - μΒ
            auto vecDist = batInput[j][i] - batOutput.batMiuBeta[i];
            // (∂L/∂ bar x)(input - μΒ)
            auto vecMiuBetaFact = Matrix::matrix::elem_mult(batCurrErr[j][i], vecDist);
            // (∂L/∂ bar x)(input - μΒ)(-((σ^2 + ε) ^ (-1.5)) / 2)
            vecGradSigmaSqr += Matrix::matrix::elem_mult(vecMiuBetaFact, vecSigmaSqrFact);
        }
        batGradSigmaSqr[i] = vecGradSigmaSqr;
    }
    // ∂L/∂μΒ
    algo_queue<Matrix::matrix> batGradMiuBeta(nChannCnt);
    for(auto i=0; i<nChannCnt; i++)
    {
        // Variance gradient normalization
        Matrix::matrix vecHeadGrad(nLnCnt, nColCnt), vecTailGrad(nLnCnt, nColCnt);
        // -√(σ^2 + ε)
        auto vecDistribute = DivisorZeroOpt(batOutput.batSigmaSqr[i], dEpsilon);
        vecDistribute.elem_power(0.5);
        vecDistribute *= -1;
        // μ, σ^2 ~ N
        for(auto j=0; j<nBatchSize; j++)
        {
            // -(∂L/∂ bar x)√(σ^2 + ε)
            vecHeadGrad += Matrix::matrix::elem_division(batCurrErr[j][i], vecDistribute);
            // distance
            auto vecDistance = batInput[j][i] - batOutput.batMiuBeta[i];
            vecDistance *= (-2);
            vecTailGrad += vecDistance;
        }
        // distance gradient
        vecTailGrad.elem_division(nBatchSize);
        auto vecGradDist = Matrix::matrix::elem_mult(batGradSigmaSqr[i], vecTailGrad);
        batGradMiuBeta[i] = vecHeadGrad + vecGradDist;
    }
    // previous error
    algo_queue<algo_queue<Matrix::matrix>> batPreErr(nBatchSize);
    for(auto i=0; i<nBatchSize; i++) batPreErr[i].init(nChannCnt);
    for(auto i=0; i<nChannCnt; i++)
    {
        // √(σ^2 + ε)
        auto vecDistribute = DivisorZeroOpt(batOutput.batSigmaSqr[i], dEpsilon);
        vecDistribute.elem_power(0.5);
        // interception - ∂L/∂μΒ
        auto vecInterception = Matrix::matrix::elem_division(batGradMiuBeta[i], nBatchSize);
        for(auto j=0; j<nBatchSize; j++)
        {
            // (∂L/∂ bar x)(distribution)
            auto vecGradDistribute = Matrix::matrix::elem_division(batCurrErr[j][i], vecDistribute);
            // distance
            auto vecDistance = batInput[j][i] - batOutput.batMiuBeta[i];
            vecDistance *= 2;
            vecDistance.elem_division(nBatchSize);
            auto vecGradDistance = Matrix::matrix::elem_mult(batGradSigmaSqr[i], vecDistance);
            batPreErr[j][i] = vecGradDistribute + vecGradDistance + vecInterception;
        }
    }
    return batPreErr;
}
algo_queue<double> ConvBNScaleGrad(algo_queue<algo_queue<Matrix::matrix>> &batCurrErr, CNNBN &batOutput)
{
    auto nLnCnt = batCurrErr[0][0].get_line(),
        nColCnt = batCurrErr[0][0].get_column();
    auto nChannCnt = batCurrErr[0].size(),
        nBatchSize = batCurrErr.size();
    algo_queue<double> batGradGamma(nChannCnt);
    for(auto i=0; i<nChannCnt; i++)
    {
        Matrix::matrix vecGradGamma(nLnCnt, nColCnt);
        for(auto j=0; j<nBatchSize; j++) vecGradGamma += Matrix::matrix::elem_mult(batCurrErr[j][i], batOutput.batNormOutput[j][i]);
        batGradGamma[i] = vecGradGamma.sum_elem();
    }
    return batGradGamma;
}
algo_queue<double> ConvBNShiftGrad(algo_queue<algo_queue<Matrix::matrix>> &batCurrErr)
{
    auto nLnCnt = batCurrErr[0][0].get_line(),
        nColCnt = batCurrErr[0][0].get_column();
    auto nChannCnt = batCurrErr[0].size(),
        nBatchSize = batCurrErr.size();
    algo_queue<double> batGradBeta(nChannCnt);
    for(auto i=0; i<batCurrErr.size(); i++)
    {
        Matrix::matrix vecGradBeta(nLnCnt, nColCnt);
        for(auto j=0; j<nBatchSize; j++) vecGradBeta += batCurrErr[j][i];
        batGradBeta[i] = vecGradBeta.sum_elem();
    }
    return batGradBeta;
}
void UpdateScaleShift(algo_queue<double> &batPreScaleShift, algo_queue<double> &batGradScaleShift, double dLearnRate)
{
    for(auto i=0; i<batPreScaleShift.size(); i++) batPreScaleShift[i] -= batPreScaleShift[i] * dLearnRate;
}
// namespace end
BATNET_END